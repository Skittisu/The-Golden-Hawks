# Data Cleaning
Three datasets are imported into a pandas data frame to perform data preprocessing. Data cleaning is done based on:
- Tokenizing is the process of splitting strings into a list of words.
- Parts of Speech (POS) tags are assigned to each word to differentiate each token as an adjective/verb/punctuation.
- Stop words are removed by providing the unwanted POS as a list.
- We have removed duplicate images,corrupted images for the image datasets.
