# Data Cleaning
All five datasets are imported into a pandas data frame to perform data preprocessing. Data cleaning is done based on:
### Text data
- Tokenizing is the process of splitting strings into a list of words.
- Parts of Speech (POS) tags are assigned to each word to differentiate each token as an adjective/verb/punctuation.
- Stop words are removed by providing the unwanted POS as a list.
### Image data
- We have removed duplicate images,corrupted images for the image datasets.
- We have checked for consistency of the image datasets
